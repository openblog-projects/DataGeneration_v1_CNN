{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb576a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd9e66c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "974452d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d59868d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer=transforms.Compose([\n",
    "    #auf 150x150 Pixel skalieren\n",
    "    transforms.Resize((150,150)),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(), # pixelrange (of each colour channel) from 0-255 to 0-; datatyp of image is now tensor,not numpy\n",
    "    transforms.Normalize([0.5,0.5,0.5], #range from 0-1 to [-1,1]\n",
    "                        [0.5,0.5,0.5])\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e494079",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path='/home/b1/CNN/DataGeneration_v1_CNN/dataset_1/train'\n",
    "test_path='/home/b1/CNN/DataGeneration_v1_CNN/dataset_1/test'\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    torchvision.datasets.ImageFolder(train_path,transform=transformer),\n",
    "    # batch size should fit to the CPU-power\n",
    "    batch_size=256, shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    torchvision.datasets.ImageFolder(test_path,transform=transformer),\n",
    "    # batch size should fit to the CPU-power\n",
    "    batch_size=256, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3544339",
   "metadata": {},
   "outputs": [],
   "source": [
    "root=pathlib.Path(train_path)\n",
    "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fe973ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'noCar']\n"
     ]
    }
   ],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c3435fe",
   "metadata": {},
   "outputs": [],
   "source": [
    " class ConvNet(nn.Module):\n",
    "        def __init__(self,num_classes=2):\n",
    "            super(ConvNet,self).__init__()\n",
    "            \n",
    "            # Input shape = (256,3,150,150); batch_size,numberOfChannels(rgb),height,width\n",
    "            # in_channels= rgb\n",
    "           \n",
    "            self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1)\n",
    "            # Output size after conv. filter ((width-kernel_size+2*Padding)/stride) +1\n",
    "            # new shape: (256,12,150,150)\n",
    "            self.bn1=nn.BatchNorm2d(num_features=12)\n",
    "            # new shape: (256,12,150,150)\n",
    "            self.relu1=nn.ReLU()\n",
    "            # new shape: (256,12,150,150)\n",
    "            self.pool=nn.MaxPool2d(kernel_size=2) # reduce the image size with factor 2\n",
    "            # new shape: (256,12,75,75)\n",
    "            \n",
    "            \n",
    "            self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)\n",
    "            # new shape: (256,20,75,75)\n",
    "            self.relu2=nn.ReLU()\n",
    "            # new shape: (256,20,75,75)\n",
    "            \n",
    "            self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "            # new shape: (256,32,75,75)\n",
    "            self.bn3=nn.BatchNorm2d(num_features=32)\n",
    "            # new shape: (256,32,75,75)\n",
    "            self.relu3=nn.ReLU()\n",
    "            # new shape: (256,32,75,75)\n",
    "            \n",
    "            # last layer:\n",
    "            self.fc=nn.Linear(in_features=32*75*75, out_features=num_classes)\n",
    "            \n",
    "            #Feed forward function\n",
    "        def forward(self,input):\n",
    "            output=self.conv1(input)\n",
    "            output=self.bn1(output)\n",
    "            output=self.relu1(output)\n",
    "                \n",
    "            output=self.pool(output)\n",
    "                \n",
    "            output=self.conv2(output)\n",
    "            output=self.relu2(output)\n",
    "                \n",
    "            output=self.conv3(output)\n",
    "            output=self.bn3(output)\n",
    "            output=self.relu3(output)\n",
    "                \n",
    "            # output will be in matrix form, with shape (256,32,75,75)\n",
    "                \n",
    "            output=output.view(-1,32*75*75) #last shape\n",
    "                \n",
    "            output=self.fc(output)\n",
    "                \n",
    "            return output\n",
    "                \n",
    "                \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7eb7002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ConvNet(num_classes=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1119fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer+loss function\n",
    "optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
    "loss_function=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d84fc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1114 201\n"
     ]
    }
   ],
   "source": [
    "num_epochs=10\n",
    "#size of training/testing images\n",
    "train_count=len(glob.glob(train_path+'/**/*.jpg'))\n",
    "test_count=len(glob.glob(test_path+'/**/*.jpg'))\n",
    "print(train_count,test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44a1b0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: tensor(0.0392) Train Accuracy: 0.9901256732495511 Test Accuracy: 0.8706467661691543\n",
      "Epoch: 1 Train Loss: tensor(0.0130) Train Accuracy: 0.9955116696588869 Test Accuracy: 0.8706467661691543\n",
      "Epoch: 2 Train Loss: tensor(0.0081) Train Accuracy: 0.9964093357271095 Test Accuracy: 0.8905472636815921\n",
      "Epoch: 3 Train Loss: tensor(0.0059) Train Accuracy: 0.9973070017953322 Test Accuracy: 0.900497512437811\n",
      "Epoch: 4 Train Loss: tensor(0.0004) Train Accuracy: 1.0 Test Accuracy: 0.900497512437811\n",
      "Epoch: 5 Train Loss: tensor(0.0003) Train Accuracy: 1.0 Test Accuracy: 0.900497512437811\n",
      "Epoch: 6 Train Loss: tensor(0.0015) Train Accuracy: 0.9991023339317774 Test Accuracy: 0.900497512437811\n",
      "Epoch: 7 Train Loss: tensor(0.0005) Train Accuracy: 1.0 Test Accuracy: 0.900497512437811\n",
      "Epoch: 8 Train Loss: tensor(6.7861e-05) Train Accuracy: 1.0 Test Accuracy: 0.900497512437811\n",
      "Epoch: 9 Train Loss: tensor(6.3991e-05) Train Accuracy: 1.0 Test Accuracy: 0.900497512437811\n"
     ]
    }
   ],
   "source": [
    "# save model with best epoch/accuracy\n",
    "\n",
    "best_accuracy=0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_accuracy=0.0\n",
    "    train_loss=0.0\n",
    "    \n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs=model(images)\n",
    "        loss=loss_function(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss+= loss.cpu().data*images.size(0)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        \n",
    "        train_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "        \n",
    "    train_accuracy=train_accuracy/train_count\n",
    "    train_loss=train_loss/train_count\n",
    "\n",
    "    # Model evaluation\n",
    "    model.eval()\n",
    "    \n",
    "    test_accuracy=0.0\n",
    "    for i, (images,labels) in enumerate(test_loader):\n",
    "        outputs=model(images)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        test_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "        \n",
    "    test_accuracy=test_accuracy/test_count\n",
    "    \n",
    "    print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy)+' Test Accuracy: '+str(test_accuracy))\n",
    "    #save model\n",
    "    if test_accuracy>best_accuracy:\n",
    "        torch.save(model.state_dict(),'best_checkpoint.model')\n",
    "        best_accuracy=test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b56c36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
